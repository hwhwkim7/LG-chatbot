import streamlit as st
from langchain.embeddings import HuggingFaceEmbeddings

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="RAG Chatbot", layout="wide")
st.title("ğŸ’¬ Washing Machine Insights Chatbot")

# ì‚¬ì´ë“œë°” ì„¤ëª…
st.sidebar.title("ğŸ”¹ About")
st.sidebar.markdown(
    """
    - **Llama-3.2-11B-Vision-Instruct-based RAG Chatbot**
    - Question-answering system using FAISS vector search
    - Data source: [EPREL](https://eprel.ec.europa.eu/screen/home)
    """
)

import functions
import torch

# ê°•ì œì ìœ¼ë¡œ ëª¨ë“  GPU ìºì‹œ ì‚­ì œ
torch.cuda.empty_cache()
torch.cuda.ipc_collect()

# ì „ì²´ GPU ë©”ëª¨ë¦¬ í• ë‹¹ í•´ì œ
for i in range(torch.cuda.device_count()):
    with torch.cuda.device(i):
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
print("âœ… CUDA ìºì‹œ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ!")
print(f"âœ… í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ GPU ID: {torch.cuda.current_device()}")
print(f"âœ… í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
print(f"âœ… í˜„ì¬ GPUì—ì„œ ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")

# load token
LLM_token, SERPER_token, _, _ = functions.load_token()

# vectorDB load ì—¬ë¶€ í™•ì¸
if "vector_db" not in st.session_state:
    # CSV ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    st.sidebar.write("ğŸ“‘ Processing CSV file...")
    df, des_dict = functions.load_data()

    # ì„ë² ë”© ëª¨ë¸ load
    st.session_state.embeddings = HuggingFaceEmbeddings(
        model_name='intfloat/e5-large-v2',
        model_kwargs={"device": "cuda"},
        encode_kwargs={"normalize_embeddings": True},
    )

    # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
    st.session_state.vector_db = functions.set_vectorDB(df, st.session_state.embeddings)
    st.session_state.df = df
    st.session_state.des_dict = des_dict
    st.sidebar.success("âœ… FAISS vector store loaded!")

# LLM ëª¨ë¸ load ì—¬ë¶€ í™•ì¸
if "model" not in st.session_state:
    st.sidebar.write("â³ Loading model...")
    model_name = "meta-llama/Llama-3.1-8B-Instruct"
    generator= functions.load_model(
        LLM_token,
        model_name = model_name
    )
    st.session_state.generator = generator
    st.sidebar.success("âœ… Model loaded!")

# chat ê¸°ë¡ ì €ì¥
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ì›¹ í˜ì´ì§€ì— chat ê¸°ë¡ ë³´ì—¬ì£¼ê¸°
for role, text in st.session_state["messages"]:
    st.chat_message(role).write(text)

# ì‚¬ìš©ì ì…ë ¥ì¹¸
query = st.chat_input("ğŸ“ Enter your question")

# ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ì‘ë‹µ ìƒì„±
if query:
    # ë©”ì‹œì§€ ì €ì¥
    st.session_state["messages"].append(("user", query))
    st.chat_message("user").write(query)

    with st.spinner("ğŸ” Searching..."):
        # ì‘ë‹µ ìƒì„±
        response = functions.generate_response(query, st.session_state.generator, st.session_state.des_dict, st.session_state.df, st.session_state.embeddings)

    st.session_state["messages"].append(("assistant", response))
    st.chat_message("assistant").write(response)
