import streamlit as st
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
import os
from dotenv import load_dotenv


# streamlit
def add_vertical_space(spaces=1):
    for _ in range(spaces):
        st.sidebar.markdown("---")


# ë°ì´í„° ë¡œë“œ
def load_data(file_name='../output/pre_data_IF.csv', des_file_name="../data/description.csv"):
    des_df = pd.read_csv(des_file_name)
    des_df.columns = ['col_name', 'description']
    des_dict = dict(zip(des_df['col_name'], des_df['description']))
    data_df = pd.read_csv(file_name)

    return data_df, des_dict

def set_vectorDB(df, db_faiss_path="vectorstore/db_faiss"):
    # Product Name: Apple | Price: 100 | Rating: 4 | Stock: 50
    df['text'] = df.apply(lambda row: " | ".join(f"{col}: {val}" for col, val in row.items()), axis=1)

    # csv ë°ì´í„°ë¥¼ listë¡œ
    documents = df['text'].tolist()

    # ë¬¸ì„œë¥¼ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸° (í•„ìš”ì— ë”°ë¼ í¬ê¸° ì¡°ì • ê°€ëŠ¥)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)
    text_chunks = text_splitter.create_documents(documents)

    # ì„ë² ë”© ëª¨ë¸ ì ìš© (HuggingFace)
    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')

    # FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    docsearch = FAISS.from_documents(text_chunks, embeddings)

    #  ë¡œì»¬ ì €ì¥ì†Œì— ì €ì¥
    os.makedirs(os.path.dirname(db_faiss_path), exist_ok=True)
    docsearch.save_local(db_faiss_path)

    # ì €ì¥ í›„ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if os.path.exists(db_faiss_path):
        print(f"âœ… FAISS ì €ì¥ì†Œê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë¨: {db_faiss_path}")
    else:
        print(f"âŒ FAISS ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨: {db_faiss_path}")

    return docsearch


# ë²¡í„° DB ìƒì„± ë˜ëŠ” ë¡œë“œ
def load_vectorDB(db_faiss_path="vectorstore/db_faiss"):
    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    if os.path.exists(db_faiss_path):
        docsearch = FAISS.load_local(db_faiss_path, embeddings, allow_dangerous_deserialization=True)
    else:
        raise FileNotFoundError(f"âŒ ë²¡í„° ì €ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_faiss_path}")
    return docsearch


# ëª¨ë¸ ë¡œë“œ
def load_model(model_name="meta-llama/Llama-3.2-11B-Vision-Instruct"):
    access_token = load_token()

    """4-bit ì–‘ìí™” ëª¨ë¸ì„ ë¡œë“œí•˜ê³  íŒŒì´í”„ë¼ì¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype="float16",
        bnb_4bit_use_double_quant=True
    )

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        config=quantization_config,
        torch_dtype=torch.float16,
        device_map="auto",
        token=access_token
    )  # GPU ìë™ ì„¤ì •

    tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)

    return model, tokenizer


# ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
def search_DB(query, k=20, db_faiss_path="vectorstore/db_faiss"):
    docsearch = load_vectorDB(db_faiss_path)
    results = docsearch.similarity_search(query, k=k)

    formatted_results = []
    extracted_models = set()

    for res in results:
        text = res.page_content
        formatted_results.append(text)

        for line in text.split("|"):
            if "modelIdentifier" in line.lower():
                model_name = line.split(":")[-1].strip()
                extracted_models.add(model_name)

    return formatted_results, extracted_models


# Llama ëª¨ë¸ ì‘ë‹µ ìƒì„±
def generate_response(query, model, tokenizer, k=20, db_faiss_path="vectorstore/db_faiss"):
    # Step 1: ë²¡í„° DB ê²€ìƒ‰
    search_results, _ = search_DB(query, k, db_faiss_path)

    # Step 2: ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…
    context = "\n".join(search_results) if search_results else "No relevant information found in the database."

    # Step 3: Llama ëª¨ë¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
    system_prompt = """
        You are a friendly chatbot that provides insights on washing machine performance based on your knowledge database.
        All user inputs will always be in Korean, and you must always respond in Korean.
        When a question is asked, analyze the column information in your vector database to determine what should be checked, and provide an appropriate response.
    """

    prompt = f"""
        [System prompt]
        {system_prompt}

        [Context - Internal DB Search]
        {context}

        [User question]
        {query}

        [Answer]
    """

    # Step 4: ë””ë°”ì´ìŠ¤ í™•ì¸ ë° ëª¨ë¸ ì…ë ¥
    device = model.device
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    # Step 5: ëª¨ë¸ ì‘ë‹µ ìƒì„±
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        temperature=0.1
    )

    # Step 6: ì‘ë‹µ ë””ì½”ë”© ë° í›„ì²˜ë¦¬
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # ğŸ”¹ `[Answer]` ì´í›„ë§Œ ì¶”ì¶œí•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„±
    if "[Answer]" in response:
        response = response.split("[Answer]")[-1].strip()

    return response


# Hugging Face í† í° ë¡œë“œ
def load_token():
    load_dotenv()
    access_token = os.getenv("HUGGINGFACE_TOKEN")
    return access_token
